{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087979d6-7e8a-429e-8ce6-7b6fd4167c12",
   "metadata": {},
   "source": [
    "# Exercise 1: Bayesian multiple shooting for latent ODEs (9 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f4821-5558-4b70-9241-5e0bfd4b9eca",
   "metadata": {},
   "source": [
    "The goal of this exercise is to observe firsthand the training instability for long trajectories that was discussed in the lecture. Specifically, we will replicate the experiment shown in Figure 1 and Appendix A of [Iakovlev et al. (2023)](https://arxiv.org/abs/2210.03466). We start by training a regular NODE with the structure given below\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "x_i &\\sim p(x_i) \\\\\n",
    "θ_{dyn} &\\sim p(\\theta_{dyn}) \\\\\n",
    "x_i &= ODEsolve(x_i, t_i, t_i, \\theta_{dyn}), i > 1 \\\\\n",
    "y_i \\mid x_i &\\sim p(y_i \\mid g(x_i, \\theta_{dec})) \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where $y_i$ are observations, $x_i$ are latent representations of said observations, $\\theta_{dyn}$ is the dynamics function, and $\\theta_{dec}$ is the decoder function. In this exercise, we will be working with a 2 state data, which means we can work in the state space directly, no need for a latent space. In other words, our decoder $\\theta_{dec}$ is the identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c82c3-9209-4a76-8010-12a7de7dae7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run this once\n",
    "#pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1896e-1010-4038-8c8c-b79380676568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Load the data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import random\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fcad75-d0b9-43b1-b9f3-cdd7efcc7466",
   "metadata": {},
   "source": [
    "## Data Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ea6c7-5b5b-47eb-a63c-f19f6b393cb8",
   "metadata": {},
   "source": [
    "We will be working with simple sequence data representing the motion of a pendulum through its angle and angular velocity. Note that because our data is simple, we will not be using a **latent** ODE in this assignment; instead our model will be operating in the state space directly. In the more complex experiments presented in Iakovlev et al. (2023), an encoder/decoder structure is needed to reduce the dimensionality of the data before modelling the system's dynamics in the latent space.\n",
    "\n",
    "Run the code in the cells below to generate a single sequence of pendulum motion (angle and angular velocity). Check that the plot looks like figure 14 in Iakovlev et al. (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68489c-5736-47ad-8b17-50aaa5d0f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pendulum_data(t_max=3.0):\n",
    "    \"\"\"\n",
    "    Generate a single 2D pendulum state trajectory [angle, velocity] using torchdiffeq solver,\n",
    "    with data sampled every 0.1 seconds as specified in the paper.\n",
    "    \n",
    "    Args:\n",
    "        t_max: End time (seconds)\n",
    "        \n",
    "    Returns:\n",
    "        data: Tensor of shape [seq_length, 2]\n",
    "        times: Tensor of shape [seq_length]\n",
    "    \"\"\"\n",
    "    # Time points every 0.1 seconds, as specified in the paper\n",
    "    t = torch.arange(0, t_max + 0.05, 0.1)  # Adding 0.05 for numerical stability\n",
    "    \n",
    "    # Initial conditions as per the paper (90 degrees position, 0 velocity)\n",
    "    angle = torch.tensor(90.0 * np.pi/180.0, dtype=torch.float32)  # 90 degrees in radians\n",
    "    velocity = torch.tensor(0.0, dtype=torch.float32)              # Initial velocity is zero\n",
    "    initial_state = torch.stack([angle, velocity])\n",
    "    \n",
    "    # Pendulum ODE function: dy/dt = [velocity, -g*sin(angle)]\n",
    "    def pendulum_dynamics(t, state):\n",
    "        angle, velocity = state[0], state[1]\n",
    "        dangle_dt = velocity\n",
    "        dvelocity_dt = -9.81 * torch.sin(angle)\n",
    "        return torch.stack([dangle_dt, dvelocity_dt])\n",
    "    \n",
    "    # Solve ODE using torchdiffeq with dopri5 solver (same as paper)\n",
    "    trajectory = odeint(\n",
    "        pendulum_dynamics, \n",
    "        initial_state, \n",
    "        t, \n",
    "        method='dopri5',  # Same solver used in the paper\n",
    "        rtol=1e-5,\n",
    "        atol=1e-5\n",
    "    )\n",
    "    \n",
    "    # trajectory shape: [seq_length, 2]\n",
    "    return trajectory, t\n",
    "\n",
    "# Function to visualize a trajectory\n",
    "def plot_pendulum_trajectory(trajectory, time):\n",
    "    \"\"\"\n",
    "    Plot a pendulum trajectory in phase space and as time series.\n",
    "    \n",
    "    Args:\n",
    "        trajectory: Shape [seq_length, 2] with angle and velocity\n",
    "        time: Shape [seq_length] with time points\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Phase space plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(trajectory[:, 0], trajectory[:, 1])\n",
    "    plt.xlabel('Angle (θ)')\n",
    "    plt.ylabel('Angular Velocity (ω)')\n",
    "    plt.title('Phase Space')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Time series plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(time, trajectory[:, 0], label='Angle (θ)')\n",
    "    plt.plot(time, trajectory[:, 1], label='Angular Velocity (ω)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Time Series')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870642e-3ee0-47d8-b31d-c56ba565ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, times = generate_pendulum_data(t_max=20.0)\n",
    "plot_pendulum_trajectory(trajectory=data, time=times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b4ed4-8c38-4293-b775-cea436bac9ac",
   "metadata": {},
   "source": [
    "## Dynamics function (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3fbc0-56f2-4207-94b0-34adb7e6fd14",
   "metadata": {},
   "source": [
    "We use a second-order dynamics function as explained in Appendix E.4.1 in Iakovlev et al. (2023). The function takes as input the position and the velocity of the pendulum and approximates their respective derivatives. The function itself is an MLP with 2 hidden layers, with hidden_dim=16 and Tanh non-linearity. \n",
    "\n",
    "Remember that latent dimension here is equal to 1, since we are working in the state space directly because our data is simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d369a3-e02d-4076-a76d-6d3c822778ed",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac94962c39d4411d84dbc05e3bf17913",
     "grade": false,
     "grade_id": "cell-17b0ea814a9fbc81",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Second-order dynamics model as described in Appendix A\n",
    "class DynamicsFunction(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        # implement \"an MLP with two hidden layers of size 16\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # the function learns the derivative of the position and velocity\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929251d2-475a-4943-99d9-640d5e89c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dynamics_function_visible():\n",
    "    \"\"\"Test the basic structure and functionality of DynamicsFunction.\"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    # Test with small dimensions for simplicity\n",
    "    latent_dim = 4\n",
    "    model = DynamicsFunction(latent_dim)\n",
    "    \n",
    "    # Check basic structure\n",
    "    assert isinstance(model, nn.Module), \"DynamicsFunction should inherit from nn.Module\"\n",
    "    assert hasattr(model, 'net'), \"DynamicsFunction should have a 'net' attribute\"\n",
    "    assert isinstance(model.net, nn.Sequential), \"The network should be implemented as nn.Sequential\"\n",
    "    \n",
    "    # Check that the model has the right number of layers\n",
    "    layers = [module for module in model.net if isinstance(module, nn.Linear)]\n",
    "    assert len(layers) == 3, \"The network should have 3 linear layers (input->hidden, hidden->hidden, hidden->output)\"\n",
    "    \n",
    "    # Test forward pass functionality\n",
    "    batch_size = 2\n",
    "    x = torch.randn(batch_size, latent_dim)\n",
    "    output = model(x)\n",
    "    \n",
    "    # Check output shape\n",
    "    assert output.shape == x.shape, f\"Output shape {output.shape} should match input shape {x.shape}\"\n",
    "    \n",
    "    # Check that the output contains the velocity unchanged in the first half\n",
    "    velocity_input = x[:, latent_dim//2:]\n",
    "    velocity_output = output[:, :latent_dim//2]\n",
    "    assert torch.allclose(velocity_input, velocity_output), \"The first half of the output should be the velocity from the input\"\n",
    "    \n",
    "    print(\"All visible tests passed!\")\n",
    "test_dynamics_function_visible()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ac58c-5460-4eb4-8229-80e6f2f85f59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "118e14dfeb0e1cb38c98b4d42b507f0f",
     "grade": true,
     "grade_id": "cell-70a307dbe1687c2d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a0746d8-fe8d-49cd-bbd8-54d57027d6ae",
   "metadata": {},
   "source": [
    "## Regular Latent NODE (2 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e1765b-948d-465a-9440-b66db12b6153",
   "metadata": {},
   "source": [
    "Now we need to implement a regular latent ODE model, to show the behavior presented in Figure 1 in Iakovlev et al. (2023).\n",
    "\n",
    "1. We want to learn both the parameters of the dynamics functio and the initial state of the sequence.\n",
    "2. In the forward step, the model integrates the dynamics function (defined earlier) to predict both the angle and the angular velocity of the pendulum. You can use torchdiffeq's odeint() to perform the integration with the following parameters: dopri5 solver, and rtol=atol=1e-5. (1 point)\n",
    "3. The model uses MSE loss to compare the predicted and true pendulum angles (note: angles alone, without velocity). (1 point)\n",
    "\n",
    "Finish implementing the class functions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12693827-ca2d-4aa5-8c97-a5e44f6552b7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40572942c7f9cd9c3fce8961d47b8ac1",
     "grade": false,
     "grade_id": "cell-888e7bfcc0d2021b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neural ODE model (simplified for Figure 1 experiment)\n",
    "class LatentNode(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim * 2  # x2 for position and velocity\n",
    "        \n",
    "        # Dynamics function\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, times):\n",
    "        # Use initial state\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def compute_loss(self, y_true, times):\n",
    "        \"\"\"Compute MSE loss as used in Figure 1 experiment\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a6fa1-417b-45e4-a323-abea44b823a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5db5746b1650f75c114c602206409129",
     "grade": true,
     "grade_id": "cell-4e5e363b39195eae",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_latent_node_visible():\n",
    "    \"\"\"Test the basic structure and functionality of LatentNode.\"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torchdiffeq import odeint\n",
    "    \n",
    "    # Initialize model with small latent dimension\n",
    "    latent_dim = 1  # This means actual dim will be 2 (position + velocity)\n",
    "    model = LatentNode(latent_dim)\n",
    "    \n",
    "    # Check basic structure\n",
    "    assert isinstance(model, nn.Module), \"LatentNode should inherit from nn.Module\"\n",
    "    assert hasattr(model, 'dynamics_func'), \"LatentNode should have a 'dynamics_func' attribute\"\n",
    "    assert isinstance(model.dynamics_func, DynamicsFunction), \"dynamics_func should be an instance of DynamicsFunction\"\n",
    "    assert hasattr(model, 'initial_state'), \"LatentNode should have an 'initial_state' attribute\"\n",
    "    assert isinstance(model.initial_state, nn.Parameter), \"initial_state should be a torch Parameter\"\n",
    "    assert model.initial_state.shape == (latent_dim * 2,), f\"initial_state should have shape ({latent_dim * 2},)\"\n",
    "    \n",
    "    # Test forward pass\n",
    "    times = torch.linspace(0, 1, 10)\n",
    "    trajectory = model(times)\n",
    "    \n",
    "    # Check output shape [time_steps, batch_size, latent_dim*2]\n",
    "    expected_shape = (len(times), 1, latent_dim * 2)\n",
    "    assert trajectory.shape == expected_shape, f\"Output shape {trajectory.shape} should match {expected_shape}\"\n",
    "    \n",
    "    # Test compute_loss method\n",
    "    y_true = torch.sin(times.unsqueeze(1))  # Simple sine wave as ground truth\n",
    "    loss = model.compute_loss(y_true, times)\n",
    "    \n",
    "    # Check that loss is a scalar\n",
    "    assert loss.ndim == 0, \"Loss should be a scalar\"\n",
    "    assert not torch.isnan(loss), \"Loss should not be NaN\"\n",
    "    assert not torch.isinf(loss), \"Loss should not be infinite\"\n",
    "    \n",
    "    print(\"All visible tests passed!\")\n",
    "    \n",
    "test_latent_node_visible()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f161f-9725-40d6-a645-9aaced28e92f",
   "metadata": {},
   "source": [
    "## Training and visualizing the loss (1 point) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf26f24-f27d-40df-9dba-24feb1844b6d",
   "metadata": {},
   "source": [
    "Complete the code below to train the Neural ODE model defined above.\n",
    "\n",
    "1. Implement the Adam optimizer with an ExponentialLR scheduler that decreases the learning rate from 3e-4 to 1e-5.\n",
    "2. Following the paper's methodology, begin training with only the first 10 points of the sequence, then double the 3. sequence length every iterations_per_length iterations.\n",
    "4. Track and store appropriate metrics during training to reproduce the visualization at the end.\n",
    "\n",
    "Your implementation should generate a loss curve similar to Figure 1 in the paper, demonstrating:\n",
    "\n",
    "1. Convergence for short sequences (length < 10)\n",
    "2. Characteristic peaks after each sequence length increase\n",
    "3. Failure to converge for sufficiently long sequences\n",
    "\n",
    "Note: The provided code is configured to train for 100 epochs, whereas the paper reports 3000 iterations per sequence length. Feel free to extend the training duration if needed, but be aware that longer training will significantly increase computation time, especially without GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16046222-ba9a-46ca-935c-69e4716a8b5d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de4afe3aca27c601f9672a9e5304156e",
     "grade": false,
     "grade_id": "cell-d81bb7f5ea372515",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_visualize_model_instability(\n",
    "    model, \n",
    "    data,\n",
    "    times,\n",
    "    iterations_per_length=3000,\n",
    "    initial_length=10,\n",
    "    print_every=100,\n",
    "    visualize=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model on sequences of increasing length to demonstrate instability\n",
    "    as shown in Figure 1 of the paper.\n",
    "    \"\"\"\n",
    "    # Initialize storage for losses and converged parameters\n",
    "    all_losses = []\n",
    "    \n",
    "    # Start with short sequence and double length until we reach full trajectory\n",
    "    seq_len = initial_length\n",
    "    seq_lengths = []\n",
    "    \n",
    "    # Calculate maximum length\n",
    "    max_length = len(times)\n",
    "    \n",
    "    # For plotting - create continuous x-axis for iterations\n",
    "    cumulative_iterations = []\n",
    "    current_iteration = 0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Plot training loss curve as in the paper (continuous line)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cumulative_iterations, all_losses)\n",
    "    \n",
    "    # Add vertical lines and labels to mark where sequence length changes\n",
    "    prev_iteration = 0\n",
    "    for i, seq_len in enumerate(seq_lengths):\n",
    "        if i > 0:  # Skip the first one since it starts at 0\n",
    "            iteration_mark = i * iterations_per_length\n",
    "            plt.axvline(x=iteration_mark, color='gray', linestyle='--', alpha=0.7)\n",
    "            plt.text(iteration_mark, 1, f'N={seq_len}', \n",
    "                rotation=0, \n",
    "                verticalalignment='bottom',\n",
    "                horizontalalignment='left',\n",
    "                transform=plt.gca().get_xaxis_transform())\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Training Loss')\n",
    "    #plt.title('Training Loss for Different Sequence Lengths')\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')  # Log scale as in the paper\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return all_losses, seq_lengths, cumulative_iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6bb40f-a099-40e7-9db7-b65d94d3a0c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89fcbfa298d772c25cf7b5902c6dd77d",
     "grade": true,
     "grade_id": "cell-e1aaa55ceb961827",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_train_and_visualize_model_instability_return_values():\n",
    "    \"\"\"Test that the function returns the expected types and structure.\"\"\"\n",
    "    # Setup a simple mock model\n",
    "    class MockModel:\n",
    "        def __init__(self):\n",
    "            self.params = [torch.nn.Parameter(torch.randn(2, 2))]\n",
    "            \n",
    "        def parameters(self):\n",
    "            return self.params\n",
    "            \n",
    "        def compute_loss(self, data, times):\n",
    "            # Mock loss that depends on sequence length to simulate NODE instability\n",
    "            # Longer sequences produce higher loss values\n",
    "            return torch.tensor(0.1 * len(data), requires_grad=True)\n",
    "    \n",
    "    model = MockModel()\n",
    "    data = torch.randn(20, 3)  # 20 time steps, 3 dimensions\n",
    "    times = torch.linspace(0, 1, 20)\n",
    "    \n",
    "    # Call function with small number of iterations for testing\n",
    "    all_losses, seq_lengths, cumulative_iterations = train_and_visualize_model_instability(\n",
    "        model, data, times, iterations_per_length=10, initial_length=5, print_every=5, visualize=False\n",
    "    )\n",
    "    \n",
    "    # Check return types\n",
    "    assert isinstance(all_losses, list), \"all_losses should be a list\"\n",
    "    assert isinstance(seq_lengths, list), \"seq_lengths should be a list\"\n",
    "    assert isinstance(cumulative_iterations, list), \"cumulative_iterations should be a list\"\n",
    "    \n",
    "    # Check that lengths match expectations\n",
    "    assert len(seq_lengths) > 0, \"seq_lengths should not be empty\"\n",
    "    assert len(all_losses) == len(cumulative_iterations), \"all_losses and cumulative_iterations should have the same length\"\n",
    "    \n",
    "    # Check sequence length progression (doubles each time)\n",
    "    for i in range(1, len(seq_lengths)):\n",
    "        assert seq_lengths[i] == seq_lengths[i-1] * 2, \"Sequence length should double each time\"\n",
    "    \n",
    "    # Check that we have the correct number of losses for each sequence length\n",
    "    iterations_per_length = 10\n",
    "    assert len(all_losses) == iterations_per_length * len(seq_lengths), \"Should have iterations_per_length losses for each sequence length\"\n",
    "    \n",
    "    print(\"All visible tests passed!\")\n",
    "\n",
    "test_train_and_visualize_model_instability_return_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d63e6-a422-4e42-9bcf-846fa7993932",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentNode(latent_dim=1)\n",
    "_ = train_and_visualize_model_instability(model, \n",
    "                                          data, \n",
    "                                          times,\n",
    "                                          iterations_per_length=100,\n",
    "                                          initial_length=10,\n",
    "                                          print_every=10,\n",
    "                                          visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ad84e-eb5c-4f41-a649-d57efe07aa7f",
   "metadata": {},
   "source": [
    "## Deriving Bayesian multiple shooting (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16cd1fe-c008-473f-bfcf-ec9f8cdecc5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Write the posterior distribution $p(\\theta_{dyn}, \\theta_{dec}, s_{1:B} \\mid y_{1:N})$ of the model using Bayesian multiple shooting as presented in the paper. Then explain how amortized inference is implemented to approximate this posterior, including:\n",
    "\n",
    "- The structure of the variational distribution $q(\\theta_{dyn}, \\theta_{dec}, s_{1:B})$\n",
    "- The role of the encoder hθenc in amortization\n",
    "- How this enables efficient parallel computation across shooting blocks\n",
    "\n",
    "2. Derive the ELBO loss used to train the multiple shooting model by:\n",
    "\n",
    "- Starting with the general variational objective\n",
    "- Expanding it to show all components (data likelihood, continuity prior, parameter priors)\n",
    "- Explaining how each term in the ELBO contributes to the model (especially highlighting how the continuity prior terms enforce trajectory smoothness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a93dc-cf89-43f6-8cf0-f546b47c0fac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "844dca0eea6058e9f55cadffdd6b9a6e",
     "grade": true,
     "grade_id": "cell-cc8c7a07fff5cc50",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6882c5f-c272-4fb0-b49a-3ec644ae902d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Latent Node with Multiple Shooting (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f6ff7-927d-4e1d-b040-ba432cecb59e",
   "metadata": {},
   "source": [
    "Implement the Neural ODE class with Bayesian sparse multiple shooting as described in the paper. Focus on these key components:\n",
    "\n",
    "1. Define shooting variables as stochastic latent variables with appropriate distributions.\n",
    "2. Partition the trajectory into a configurable number of blocks (num_blocks), creating efficient sub-trajectories.\n",
    "3. For each block, compute the trajectory segment by integrating the dynamics function from the corresponding shooting variable.\n",
    "4. Construct the ELBO loss with three essential terms: Reconstruction loss (data likelihood), KL divergence for the first shooting variable (initial state prior), Continuity KL divergence (enforcing smoothness between blocks)\n",
    "\n",
    "When trained properly, this model should demonstrate stable convergence across increasing sequence lengths, unlike the standard Neural ODE implementation. Note that the loss scale is much larger here than the MSE loss, as long as the pattern shows clear decrease your implementation should be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405091a-085f-4771-8225-9d38390802de",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a521f3e7baa087925eeb77f26ddf2138",
     "grade": false,
     "grade_id": "cell-3241d810397f456e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multiple shooting version of the LatentNODE model with Sparse Bayesian approach\n",
    "class MSLatentNODE(nn.Module):\n",
    "    def __init__(self, latent_dim=2, num_blocks=5):\n",
    "        \"\"\"\n",
    "        Multiple shooting version of the LatentNODE model with Sparse Bayesian approach.\n",
    "        \n",
    "        Args:\n",
    "            latent_dim: Dimension of the latent space (per coordinate)\n",
    "            num_blocks: Number of shooting blocks to use\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim * 2  # x2 for position and velocity\n",
    "        self.num_blocks = num_blocks\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def sample_shooting_vars(self):\n",
    "        \"\"\"\n",
    "        Sample shooting variables from their variational distributions.\n",
    "        This implements the reparameterization trick for backpropagation.\n",
    "        \"\"\"\n",
    "        shooting_vars = []\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return shooting_vars\n",
    "    \n",
    "    def forward(self, times):\n",
    "        \"\"\"\n",
    "        Forward pass using multiple shooting with sampled shooting variables.\n",
    "        \n",
    "        Args:\n",
    "            times: Time points for evaluation\n",
    "            \n",
    "        Returns:\n",
    "            Complete trajectory by stitching together sub-trajectories\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _get_block_boundaries(self, times):\n",
    "        \"\"\"\n",
    "        Determine the start and end indices for each block.\n",
    "        Creates blocks with approximately equal size by default.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def compute_continuity_kl_divergence(self, times):\n",
    "        \"\"\"\n",
    "        Compute KL divergence for the continuity prior as in the paper.\n",
    "        This is one of the key innovations in the paper's approach.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def compute_loss(self, y_true, times):\n",
    "        \"\"\"\n",
    "        Compute total loss following the paper's ELBO formulation:\n",
    "        ELBO = reconstruction term - KL divergence terms\n",
    "        \n",
    "        Args:\n",
    "            y_true: Ground truth trajectory\n",
    "            times: Time points\n",
    "            \n",
    "        Returns:\n",
    "            Negative ELBO (loss to minimize)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d1cec-df1d-44d3-b987-c29f804f8310",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "654ada290bd75235832cbdb30e4c5589",
     "grade": true,
     "grade_id": "cell-02ac3bb03c40acac",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_mslatentnode_initialization():\n",
    "    \"\"\"Test that the MSLatentNODE model initializes correctly.\"\"\"\n",
    "    # Create model with default parameters\n",
    "    latent_dim = 2\n",
    "    num_blocks = 5\n",
    "    model = MSLatentNODE(latent_dim=latent_dim, num_blocks=num_blocks)\n",
    "    \n",
    "    # Check model attributes\n",
    "    assert model.latent_dim == latent_dim * 2, \"Latent dimension should be doubled for position and velocity\"\n",
    "    assert model.num_blocks == num_blocks, \"Number of blocks should match input\"\n",
    "    \n",
    "    # Check that shooting variables are initialized\n",
    "    assert len(model.shooting_means) == num_blocks, \"There should be one mean parameter per block\"\n",
    "    assert len(model.shooting_log_stds) == num_blocks, \"There should be one log_std parameter per block\"\n",
    "    \n",
    "    # Check that first shooting variable is initialized to pendulum initial conditions\n",
    "    assert abs(model.shooting_means[0][0].item() - (np.pi/2.0)) < 1e-5, \"First shooting mean should be initialized to π/2 (90 degrees)\"\n",
    "    assert abs(model.shooting_means[0][latent_dim].item()) < 1e-5, \"First shooting velocity should be initialized to 0\"\n",
    "    \n",
    "    print(\"Initialization test passed!\")\n",
    "\n",
    "test_mslatentnode_initialization()\n",
    "\n",
    "def test_mslatentnode_block_boundaries():\n",
    "    \"\"\"Test that block boundaries are correctly calculated.\"\"\"\n",
    "    model = MSLatentNODE(latent_dim=2, num_blocks=3)\n",
    "    times = torch.linspace(0, 1, 10)\n",
    "    \n",
    "    # Get block boundaries\n",
    "    boundaries = model._get_block_boundaries(times)\n",
    "    \n",
    "    # Check that we have the correct number of boundaries\n",
    "    assert len(boundaries) == 3, \"Should have 3 boundary pairs for 3 blocks\"\n",
    "    \n",
    "    # Check that the boundaries cover the entire range\n",
    "    assert boundaries[0][0] == 0, \"First block should start at index 0\"\n",
    "    assert boundaries[-1][1] == len(times), \"Last block should end at the last time point\"\n",
    "    \n",
    "    # Check that blocks are contiguous\n",
    "    for i in range(len(boundaries)-1):\n",
    "        assert boundaries[i][1] == boundaries[i+1][0], \"Blocks should be contiguous\"\n",
    "    \n",
    "    print(\"Block boundaries test passed!\")\n",
    "    \n",
    "test_mslatentnode_block_boundaries()\n",
    "\n",
    "def test_mslatentnode_forward():\n",
    "    \"\"\"Test that the forward pass produces output of the correct shape.\"\"\"\n",
    "    latent_dim = 2\n",
    "    num_blocks = 3\n",
    "    model = MSLatentNODE(latent_dim=latent_dim, num_blocks=num_blocks)\n",
    "    \n",
    "    # Create some time points\n",
    "    times = torch.linspace(0, 1, 20)\n",
    "    \n",
    "    # Run forward pass\n",
    "    trajectory = model(times)\n",
    "    \n",
    "    # Check output shape\n",
    "    expected_shape = (len(times), 1, latent_dim * 2)  # (time, batch, features)\n",
    "    assert trajectory.shape == expected_shape, f\"Expected shape {expected_shape}, got {trajectory.shape}\"\n",
    "    \n",
    "    print(\"Forward pass test passed!\")\n",
    "\n",
    "test_mslatentnode_forward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba4de3-4b45-428e-8506-86aaadb4520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MSLatentNODE(latent_dim=1)\n",
    "_ = train_and_visualize_model_instability(model, \n",
    "                                          data, \n",
    "                                          times,\n",
    "                                          iterations_per_length=100,\n",
    "                                          initial_length=10,\n",
    "                                          print_every=10,\n",
    "                                          visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0f0c4-4d95-45d1-980f-565a4a808102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
